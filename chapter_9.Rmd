---
title: "chapter_9"
author: "Jrose"
date: '2022-10-01'
output: html_document
---

# Chapter 9: Judging Model Effectiveness

A quantitative approach for estimating model effectiveness allows you to compare different models, or tweak a model to improve performance.

We will focus on empirical validation which usually means using data that were not used to create teh model to assess effectiveness

Which metric you use for optimization of a model can have unintended consequences.

  I.e. RMSE measures accuracy while R^2 measures correlation
  
We'll be using the `yardstick` package in this chapter.

## Set up

Code from previous chapter:

```{r}
library(tidymodels)
data(ames)
ames <- mutate(ames, Sale_Price = log10(Sale_Price))

set.seed(502)
ames_split <- initial_split(ames, prop = 0.80, strata = Sale_Price)
ames_train <- training(ames_split)
ames_test  <-  testing(ames_split)

ames_rec <- 
  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + 
           Latitude + Longitude, data = ames_train) %>%
  step_log(Gr_Liv_Area, base = 10) %>% 
  step_other(Neighborhood, threshold = 0.01) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_interact( ~ Gr_Liv_Area:starts_with("Bldg_Type_") ) %>% 
  step_ns(Latitude, Longitude, deg_free = 20)
  
lm_model <- linear_reg() %>% set_engine("lm")

lm_wflow <- 
  workflow() %>% 
  add_model(lm_model) %>% 
  add_recipe(ames_rec)

lm_fit <- fit(lm_wflow, ames_train)
```

### 9.1 Performance Metrics and Inference

Effetiveness is determined by the ultimate use of a model (inference vs prediction)

This chapter mostly focuses on maximizing predictive strength, however the authors advise those developing inferential models to use the same techniques.

It is also important that a model closely fits actual data if you want to make any use for inference out of it!

### 9.2 Regression Metrics

Yardstick package functions have the general syntax:

  `function(data, truth, ...)`
  
Where data is a dataframe, truth is the columnwith teh observed outcome values

Going back to our AMES model from chapter 8 
```{r}
ames_test_res <- predict(lm_fit, new_data = ames_test %>% select(-Sale_Price))
ames_test_res
```

Let's compare the predictions with their observed truth values.
```{r}
ames_test_res <- bind_cols(ames_test_res, ames_test %>% select(Sale_Price))
ames_test_res
```

And plot the data

```{r}
ggplot(ames_test_res, aes(x = Sale_Price, y = .pred)) + 
  # Create a diagonal line:
  geom_abline(lty = 2) + 
  geom_point(alpha = 0.5) + 
  labs(y = "Predicted Sale Price (log10)", x = "Sale Price (log10)") +
  # Scale and size the x- and y-axis uniformly:
  coord_obs_pred()
```

We can compute RMSE like so:

```{r}
rmse(ames_test_res, truth = Sale_Price, estimate = .pred)
```

And we can compute multiple metrics using `metric_set()`

```{r}
ames_metrics <- metric_set(rmse, rsq, mae)
ames_metrics(ames_test_res, truth = Sale_Price, estimate = .pred)
```
Yardstick doesn't use adjusted R^2 because it is commonly used to evaluate a model using the SAME data used to create it. Somethign they don't want to encourage.

### 9.3 Binary Classiication Metrics

