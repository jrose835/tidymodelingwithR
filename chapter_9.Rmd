---
title: "chapter_9"
author: "Jrose"
date: '2022-10-01'
output: html_document
---

# Chapter 9: Judging Model Effectiveness

A quantitative approach for estimating model effectiveness allows you to compare different models, or tweak a model to improve performance.

We will focus on empirical validation which usually means using data that were not used to create teh model to assess effectiveness

Which metric you use for optimization of a model can have unintended consequences.

  I.e. RMSE measures accuracy while R^2 measures correlation
  
We'll be using the `yardstick` package in this chapter.

## Set up

Code from previous chapter:

```{r}
library(tidymodels)
data(ames)
ames <- mutate(ames, Sale_Price = log10(Sale_Price))

set.seed(502)
ames_split <- initial_split(ames, prop = 0.80, strata = Sale_Price)
ames_train <- training(ames_split)
ames_test  <-  testing(ames_split)

ames_rec <- 
  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + 
           Latitude + Longitude, data = ames_train) %>%
  step_log(Gr_Liv_Area, base = 10) %>% 
  step_other(Neighborhood, threshold = 0.01) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_interact( ~ Gr_Liv_Area:starts_with("Bldg_Type_") ) %>% 
  step_ns(Latitude, Longitude, deg_free = 20)
  
lm_model <- linear_reg() %>% set_engine("lm")

lm_wflow <- 
  workflow() %>% 
  add_model(lm_model) %>% 
  add_recipe(ames_rec)

lm_fit <- fit(lm_wflow, ames_train)
```

### 9.1 Performance Metrics and Inference

Effectiveness is determined by the ultimate use of a model (inference vs prediction)

This chapter mostly focuses on maximizing predictive strength, however the authors advise those developing inferential models to use the same techniques.

It is also important that a model closely fits actual data if you want to make any use for inference out of it!

### 9.2 Regression Metrics

Yardstick package functions have the general syntax:

  `function(data, truth, ...)`
  
Where data is a dataframe, truth is the columnwith teh observed outcome values

Going back to our AMES model from chapter 8 
```{r}
ames_test_res <- predict(lm_fit, new_data = ames_test %>% select(-Sale_Price))
ames_test_res
```

Let's compare the predictions with their observed truth values.
```{r}
ames_test_res <- bind_cols(ames_test_res, ames_test %>% select(Sale_Price))
ames_test_res
```

And plot the data

```{r}
ggplot(ames_test_res, aes(x = Sale_Price, y = .pred)) + 
  # Create a diagonal line:
  geom_abline(lty = 2) + 
  geom_point(alpha = 0.5) + 
  labs(y = "Predicted Sale Price (log10)", x = "Sale Price (log10)") +
  # Scale and size the x- and y-axis uniformly:
  coord_obs_pred()
```

We can compute RMSE like so:

```{r}
rmse(ames_test_res, truth = Sale_Price, estimate = .pred)
```

And we can compute multiple metrics using `metric_set()`

```{r}
ames_metrics <- metric_set(rmse, rsq, mae)
ames_metrics(ames_test_res, truth = Sale_Price, estimate = .pred)
```
Yardstick doesn't use adjusted R^2 because it is commonly used to evaluate a model using the SAME data used to create it. Somethign they don't want to encourage.

### 9.3 Binary Classiication Metrics

Let's look at another example data set

```{r}
data(two_class_example)
tibble(two_class_example)
```

Yardstick functions for class predictions

```{r}
conf_mat(two_class_example, truth=truth, estimate=predicted)
```

```{r}
#Accuracy
accuracy(two_class_example, truth, predicted)

# Matthews correlation coefficient:
mcc(two_class_example, truth, predicted)

# F1 metric:
f_meas(two_class_example, truth, predicted)

# Combining these three classification metrics together
classification_metrics <- metric_set(accuracy, mcc, f_meas)
classification_metrics(two_class_example, truth = truth, estimate = predicted)

```
Be careful to check which level (first or second in binary) is considered the "event" by the metrics used. It varies. 

Tidymodels has roc functions:

`roc_curve()` which calculates the datapoints of the roc curve

and `roc_auc()` which calculates the area under the roc curve.

```{r}
two_class_curve <- roc_curve(two_class_example, truth, Class1)
two_class_curve

roc_auc(two_class_example, truth, Class1)
```

There's even a cool `autoplot()` function, but you can also make a ggplot from the roc_curve() output

```{r}
autoplot(two_class_curve)
```

### 9.4 Multiclass Classification Metrics

What about data with more classes??

```{r}
data(hpc_cv)
tibble(hpc_cv)
```

You can use the same discrete calss prediction metrics as above

```{r}
accuracy(hpc_cv, obs, pred)
mcc(hpc_cv, obs, pred)
```

Many of these methods were designed for two classes but can be extended to outcomes with more than two classes.

Wrappers for things like sensitivity exist but need another arugment:

* Macro-average computers one-v-all metric and then averages

* Maco-weighted does the same but the average is weighted by the number of samples in each class

* Micro-averaging computesr the contribution for each class, aggregates them, and then computers a sigle metric from the aggregates.

```{r}
sensitivity(hpc_cv, obs, pred, estimator = "macro")
sensitivity(hpc_cv, obs, pred, estimator = "macro_weighted")
sensitivity(hpc_cv, obs, pred, estimator = "micro")

```
Muli-class ROC

*Must pass the names of all class probability columns*

```{r}
roc_auc(hpc_cv, obs, VF, F, M, L)
roc_auc(hpc_cv, obs, VF, F, M, L, estimator = "macro_weighted")
```

You can also use dplyr grouping

```{r}
hpc_cv %>% 
  group_by(Resample) %>% 
  accuracy(obs, pred)
```

```{r}
hpc_cv %>% 
  group_by(Resample) %>% 
  roc_curve(obs, VF, F, M, L) %>% 
  autoplot()
```

### 9.5 Chapeter Summary

* Different metrics measure different aspects of model fit

* It is important to measure model performance even when prediction is not your goal

* Yardstick package has lots of functions for metrics

* Different metrics for regression and classification are available
