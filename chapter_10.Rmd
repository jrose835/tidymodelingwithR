---
title: "chapter_10"
author: "Jrose"
date: '2022-10-31'
output: html_document
---

# Chapter 10: Resampling for evaluating performance

Its hard to decide which model to use with the "test" data when comparing multiple models.

Resampling fills the gap for assessing performance between train and testing data splits.

Set up from last chapter:

```{r}
library(tidymodels)
data(ames)
ames <- mutate(ames, Sale_Price = log10(Sale_Price))

set.seed(502)
ames_split <- initial_split(ames, prop = 0.80, strata = Sale_Price)
ames_train <- training(ames_split)
ames_test  <-  testing(ames_split)

ames_rec <- 
  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + 
           Latitude + Longitude, data = ames_train) %>%
  step_log(Gr_Liv_Area, base = 10) %>% 
  step_other(Neighborhood, threshold = 0.01) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_interact( ~ Gr_Liv_Area:starts_with("Bldg_Type_") ) %>% 
  step_ns(Latitude, Longitude, deg_free = 20)
  
lm_model <- linear_reg() %>% set_engine("lm")

lm_wflow <- 
  workflow() %>% 
  add_model(lm_model) %>% 
  add_recipe(ames_rec)

lm_fit <- fit(lm_wflow, ames_train)
```

### 10.1 The Resubstitution Approach

First we have to look at why resubsitution doesn't work

*Resubstituting* is when you measure performance on the same data used to train a model

To compare to the linear model from chapter 8 (above) let's create a random forest model

```{r}
rf_model <- 
  rand_forest(trees = 1000) %>% 
  set_engine("ranger") %>% 
  set_mode("regression")

rf_wflow <- 
  workflow() %>% 
  add_formula(
    Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + 
      Latitude + Longitude) %>% 
  add_model(rf_model) 

rf_fit <- rf_wflow %>% fit(data = ames_train)
```

Random forest models require no preprocessing...which is kinda nice

How should we compare the two models??

Let's calculate some statistics using resubstitution:

```{r}
estimate_perf <- function(model, dat) {
  # Capture the names of the `model` and `dat` objects
  cl <- match.call()
  obj_name <- as.character(cl$model)
  data_name <- as.character(cl$dat)
  data_name <- gsub("ames_", "", data_name)
  
  # Estimate these metrics:
  reg_metrics <- metric_set(rmse, rsq)
  
  model %>%
    predict(dat) %>%
    bind_cols(dat %>% select(Sale_Price)) %>%
    reg_metrics(Sale_Price, .pred) %>%
    select(-.estimator) %>%
    mutate(object = obj_name, data = data_name)
}

estimate_perf(rf_fit, ames_train)
estimate_perf(lm_fit, ames_train)
```

It looks like the rf model is better here based on rmse

Let's evaluate it on the test set:

```{r}
estimate_perf(rf_fit, ames_test)
```

It's much worse! Why? Because rf models are **low bias** which means we've overfitted the training data.

Linear model doesn't have as much of an issue regarding this because it is less complex

**Main take away:** Repredicting the training data will result in an artificially optimistic estimate of performance

What should we do? Resampling using methods like cross-validation or validation sets.

### 10.2 Resampling Methods