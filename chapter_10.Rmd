---
title: "chapter_10"
author: "Jrose"
date: '2022-10-31'
output: html_document
---

# Chapter 10: Resampling for evaluating performance

Its hard to decide which model to use with the "test" data when comparing multiple models.

Resampling fills the gap for assessing performance between train and testing data splits.

Set up from last chapter:

```{r}
library(tidymodels)
data(ames)
ames <- mutate(ames, Sale_Price = log10(Sale_Price))

set.seed(502)
ames_split <- initial_split(ames, prop = 0.80, strata = Sale_Price)
ames_train <- training(ames_split)
ames_test  <-  testing(ames_split)

ames_rec <- 
  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + 
           Latitude + Longitude, data = ames_train) %>%
  step_log(Gr_Liv_Area, base = 10) %>% 
  step_other(Neighborhood, threshold = 0.01) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_interact( ~ Gr_Liv_Area:starts_with("Bldg_Type_") ) %>% 
  step_ns(Latitude, Longitude, deg_free = 20)
  
lm_model <- linear_reg() %>% set_engine("lm")

lm_wflow <- 
  workflow() %>% 
  add_model(lm_model) %>% 
  add_recipe(ames_rec)

lm_fit <- fit(lm_wflow, ames_train)
```

### 10.1 The Resubstitution Approach

First we have to look at why resubsitution doesn't work

*Resubstituting* is when you measure performance on the same data used to train a model

To compare to the linear model from chapter 8 (above) let's create a random forest model

```{r}
rf_model <- 
  rand_forest(trees = 1000) %>% 
  set_engine("ranger") %>% 
  set_mode("regression")

rf_wflow <- 
  workflow() %>% 
  add_formula(
    Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + 
      Latitude + Longitude) %>% 
  add_model(rf_model) 

rf_fit <- rf_wflow %>% fit(data = ames_train)
```

Random forest models require no preprocessing...which is kinda nice

How should we compare the two models??

Let's calculate some statistics using resubstitution:

```{r}
estimate_perf <- function(model, dat) {
  # Capture the names of the `model` and `dat` objects
  cl <- match.call()
  obj_name <- as.character(cl$model)
  data_name <- as.character(cl$dat)
  data_name <- gsub("ames_", "", data_name)
  
  # Estimate these metrics:
  reg_metrics <- metric_set(rmse, rsq)
  
  model %>%
    predict(dat) %>%
    bind_cols(dat %>% select(Sale_Price)) %>%
    reg_metrics(Sale_Price, .pred) %>%
    select(-.estimator) %>%
    mutate(object = obj_name, data = data_name)
}

estimate_perf(rf_fit, ames_train)
estimate_perf(lm_fit, ames_train)
```

It looks like the rf model is better here based on rmse

Let's evaluate it on the test set:

```{r}
estimate_perf(rf_fit, ames_test)
```

It's much worse! Why? Because rf models are **low bias** which means we've overfitted the training data.

Linear model doesn't have as much of an issue regarding this because it is less complex

**Main take away:** Repredicting the training data will result in an artificially optimistic estimate of performance

What should we do? Resampling using methods like cross-validation or validation sets.

### 10.2 Resampling Methods

Most resampling methods are iterative. They split the training data further into multiple sets of analysis and assessment splits and compare by fitting multiple models on each segment of data.

### 10.2.1 Cross validation

Most common is V-fold cross valiation. Data are randomly partitioned into V sets of roughly equal size (called *folds*). 

For each iteration, one fold is held out for assessment and the remaining folds are used to train the model.

V is often 5 or 10 in practice, 10 is best for most situations. 

```{r}
set.seed(1001)
ames_folds <- vfold_cv(ames_train, v = 10)
ames_folds
```
#### Repeated cross-validation

"Depending on data size and other characteristics, the resampling estimate procued by V-fold cross-validation may be excessively noisy"

You can repeat cross-validation to collect more data and reduce this noise

```{r}
vfold_cv(ames_train, v = 10, repeats = 5)
```
#### Leave-one-out cross validation

For n training set samples, n models are fit using n-1 rows of the training set. The n predictions are pooled at the end to produce a single performance statistic.

LOO methods are generally only good for extremely small samples. Not part of tidymodels.

#### Monte carlo cross-validation

Similar to cross-validation but the splits are completely random each time and therefore not mutually exclusive.

```{r}
mc_cv(ames_train, prop = 9/10, times = 20)
```

### 10.2.2 Validation Sets

Initial available data is split into training, validation, and test sets.

Often used when original pool of data is VERY LARGE

```{r}
set.seed(1002)
val_set <- validation_split(ames_train, prop = 3/4)
val_set
```

### 10.2.3 Bootstrapping

Originally designed as a method to approximate sample distributions from unknown distributions.

A bootstrap smaple of the training data is a sample that is the same size as the training data set but is drawn *with replacement* so that some samples can be selected multiple times. 

The assessment set is made up of those samples which were not selected for analysis (also called out-of-bag sample).

```{r}
bootstraps(ames_train, times = 5)
```

Bootstrap procedure generates estimates with very **low variance** but often with a significant **pessimistic bias**


### 10.2.4 Rolling Forcasting Origin 

Data with a time-component may have *seasonal or temporal trends* which should be reflected in the resampling.

```{r}
time_slices <- 
  tibble(x = 1:365) %>% 
  rolling_origin(initial = 6 * 30, assess = 30, skip = 29, cumulative = FALSE)

data_range <- function(x) {
  summarize(x, first = min(x), last = max(x))
}

map_dfr(time_slices$splits, ~   analysis(.x) %>% data_range())
map_dfr(time_slices$splits, ~ assessment(.x) %>% data_range())
```

### 10.3 Estimating Performance